
\subsection{Introduction to convex optimization problems}
General form: Consider the functions $F_i(x), h_i(x): \reals^n\rightarrow \reals$.
\begin{align*}
\min_{x\in \reals^n} \quad &F_0(x) \quad \text{"objective function"}\\
s.t. \quad &F_i(x) \leq 0, i = 1...m \quad\text{inequality constraints}\\
&h_i(x) =0,i =1...p \quad \text{equality constraints}
\end{align*}


Feasible set for this question:
$$\mathcal{C} = \{x\vert F_i(x) \leq 0,i=1...m,h_i(x) = 0,i = 1...p \}$$


Optimal value: $p^* = \inf_{x\in \mathcal{C}}F_0(x)$. Note that it could be $\infty$, and also could be empty.

Optimal points: $\{x\in \mathcal{C}\vert F_0(x) = p^* \}$. Note that it could be empty, and also could be not unique.



\begin{example}
Consider the optimization problem:
\begin{align*}
\min_x \quad &\min x_1 + x_2 \\
s.t. 
&-x_1\leq 0\\
&-x_2\leq 0\\
&1- x_1 x_2\leq 0\\
\end{align*}

So $x^*=
\begin{bmatrix}
1\\
1
\end{bmatrix}$
and $p^* = 2$, as illustrated in the figure.

\end{example}


\vspace{0.5cm}
\textbf{Convex optimization problem: }
\begin{align*}
\min_x \quad &F_0(x) \\
s.t. &F_i(x)\leq 0 \quad i = 1,\cdots,m\\
&a^T_i x - b_i = 0 \quad i =i,\cdots,p\\
\end{align*}
$a^T_i + b_i = 0$ is often written as:
$$
\begin{bmatrix}
a_1^T\\
a_2^T\\
\vdots\\
a_p^T
\end{bmatrix}x = 
\begin{bmatrix}
b_1\\
\vdots\\
b_P
\end{bmatrix}
\Leftrightarrow
Ax=b
$$

\begin{enumerate}
	\item All $F_i$, $i\in \{0,1,...n \}$ are convex functions.
	\item All equality constraints are affine.
\end{enumerate}


%\begin{example}
%	\begin{align*}
%	&\min\quad{x_1+x_2}\\
%	s.t. &-x_1\leq 0\\
%	&-x_2\leq 0\\
%	&1-x_1x_2 \leq 0\\
%	\end{align*}
%	
%	GRAPH7
%\end{example}

\textbf{Remarks:}
\begin{enumerate}
	\item Think about feasible set,
	\begin{equation*}
	\mathcal{C} = ( \cap^m_{i=1}\{x\vert F_i(x) \leq 0 \} ) \cap (\cap^p_{i=1}\{x\vert a_i^Tx - b_i = 0 \})
	\end{equation*}
	
	For the first part, each is a sublevel set of a convex function therefore convex.
	
	For the second part, each is an affine set and therefore convex.
	
    So the feasible set $\mathcal{C}$ is an intersection of $p+m$ convex sets, and therefore it is a convex set.
	
	\item Note: $h_i(x)$ are affine(and not more general convex) to keep the set $\{x\vert h_i(x) = 0 \}$ a convex set. 
	
	Let $h_i(x) = x^2 - 1$: 
	\begin{equation*}
	\{x\vert x^2 - 1 = 0 \} = \{x\vert x^2 = 1 \} = \{\pm 1 \}
	\end{equation*}
	\begin{marginfigure}
	\centering
	\includegraphics[width=1.8in,height=1.8in]{figures/ch08/figure1111_6.png}
	%\caption{This is an inserted JPG graphic} 
	%\label{fig:graph} 
	\end{marginfigure}
\end{enumerate}

%Above are notes for Nov 11


%Below are notes for Nov 13

\begin{align*}
\min_x\quad & F_0(x) \\
s.t.\quad & F_i(x) \leq 0 \quad i = 1,...,m\\
& a_i^Tx - b_i = 0\quad i = 1,...,p
\end{align*}

\begin{itemize}
	\item $F_o, F_1,...,F_m$ are convex
	
	\item $Ax - b = 0$
\end{itemize}

\begin{definition}
	$x\in \mathcal{C}$ is locally optimal for a constrained optimization if $\exists \epsilon > 0$, $s.t.\forall y\in \mathcal{C}$ and $\Vert x-y\Vert < \epsilon$, we have $F_0(y) \geq F_0(x)$
\end{definition}

\begin{theorem}
	For a convex optimization problem a local minimum is global optimization. 
\end{theorem}
Proved aparticular instance of this for:
\begin{enumerate}
	\item Unconstrained optimization
	
	\item Differentiable $F_0$
\end{enumerate}

\begin{proof}
	(Proof by contradiction)
	
	Suppose $x\in \mathcal{C}$ is not globally optimal but is locally minimal.
	
	$\rightarrow$ Because not globally optimal, $\exists y\in \mathcal{C}$ s.t. $F_0(y)<F_0(x)$
	
	$\rightarrow$ Consider $z = \lambda x + (1-\lambda)y\in \mathcal{C}$ because $\mathcal{C}$ is convex.
	
	\begin{align*}
	F_0(z) &\leq \lambda F_0(x) + (1-\lambda)F_0(x)\\
	&< \lambda F_0(x) + (1-\lambda)F_0(x)\\
	&= F_0(x)
	\end{align*}
	
	$\rightarrow$ By picking $\lambda$ sufficiently close to 1(but $<$ 1), $z\in \mathcal{C}$ is in neighborhood of $x$ and has lower cost so $x$ cannot be local minimum. $\Rightarrow$ Contradiction
\end{proof}

As for: 

\begin{align*}
\min_x\quad & F_0(x) \\
s.t.\quad & F_i(x) \leq 0 \quad i = 1,...,m\\
& a_i^Tx - b_i = 0\quad i = 1,...,p
\end{align*}


For differentiable convex $F_0$:

\begin{itemize}
	\item For unconstrained $x^*$ is optimal iff $\triangledown F(x^*) = 0$
	
	\item For constrained optimization very possible no $x\in \mathcal{C}$ satisfies $\triangledown F(x) = 0$
\end{itemize}

\begin{theorem}
	For a convex optimization problem with (convex) feasible set $\mathcal{C}$ and differentiable (convex) objective $F_0: \Re^n \rightarrow \Re_i$, a point $x^* \in \mathcal{C}$ is optimal iff:
	
	\begin{equation*}
	\triangledown F_0(x)^T(y - x^*) \geq 0  \quad \forall y \in \mathcal{C}
	\end{equation*}
\end{theorem}

Start at positive optimum $x^*\in \mathcal{C}$, move into feasible set in direction $v$ and evaluate, $F_0(x^*+tv)$ must be non-decreasing for $t\geq 0$.

\begin{marginfigure}
\centering
\includegraphics[width=1.8in,height=1.8in]{figures/ch09/figure1113_1.png}
%\caption{This is an inserted JPG graphic} 
%\label{fig:graph} 
\end{marginfigure}

\begin{proof}
	First assume $x^*$ is the global optimum, apply $1^{st}$-order condition for optimality, i.e. $\forall y \in \mathcal{C}$:
	
	\begin{align*}
	F_0(y) &\geq F_0(x^*) + \triangledown F_0(x^*)^T(y-x^*)\\
	&\geq F_0(x^*)
	\end{align*}
	$\rightarrow$ Basically same prove now $2^{nd}$ term $\geq 0$(not = 0)
	
	Second: 
	
	If $x^*$ is global optimum, suppose $x^*$ is global optimum and $\exists y\in \mathcal{C}$s.t. $\triangledown F_0(x^*)^T(y-x^*)<0$.
	
	Look at points 
	
	\begin{align*}
	z &= \lambda y + (1-\lambda)x^*\\
	&= x^* + \lambda(y-x^*)\quad \lambda \in [0,1]
	\end{align*}
	Feasible $\forall \lambda$ because $\mathcal{C}$ is convex set.
	
	\begin{align*}
	\left.\frac{dF_0(z)}{d\lambda}\right|_{\lambda = 0} &=\left.\frac{d}{d\lambda}F_0(x^*+\lambda(y-x^*))\right|_{\lambda = 0}\\
	&= \triangledown F_0(x^*)^T(y-x^*)\\
	&< 0
	\end{align*}
	
	Since slope is strictly negative, as increasing $\lambda>0$, initially $F_0(z)$ decreases $\rightarrow$ contradicts global optimality of $x^*$, therefore have a contradiction. 
\end{proof}


\subsection{Quasi-convex minimization}

Recall: $F_0$ is quasi-convex if all its sub-level sets are convex sets

Problem:


\begin{align*}
\min_x\quad & F_0(x) \quad \leftarrow \text{quasi-convex}\\
s.t.\quad & F_i(x) \leq 0 \quad i = 1,...,m \quad \leftarrow \text{convex}\\
& a_i^T - b_i = 0\quad i = 1,...,m \quad \leftarrow \text{affine}
\end{align*}

can be written as:


\begin{align*}
\min_x\quad & F_0(x) \quad \leftarrow \text{convex set}\\
s.t.\quad  & F_0(x) \leq t  \\
& F_i(x) \leq 0 \quad i = 1,...,m \\
& a_i^T - b_i = 0\quad i = 1,...,m
\end{align*}
which is a feasible problem

\begin{example}
	$F_0(x) = \frac{P(x)}{Q(x)}$ where $p(x)$ is convex, $q(x)$ is concave, $domF_0 = \{x\vert Q(x) >0 \}$.
	
	\begin{align*}
	\{x\vert F_0(x) \leq t \} &= \{x\vert \frac{P(x)}{Q(x)}\leq t \}\\
	&= \{x\vert P(x)\leq tQ(x) \}\\
	&= \{x\vert P(x) - tQ(x)\leq 0 \}\\
	f_0(x) &= \frac{a^Tx + b}{c^Tx+d}
	\end{align*}
	
	If the $P(x) - tQ(x)$ is convex, then $f_0$ us quasi-convex.
	
	\begin{itemize}
		\item super-level set convex $\Rightarrow$ quasi-concave
		
		\item sub-level set convex $\Rightarrow$ quasi-convex
	\end{itemize}
	
	Quasi-convex promises a global optimal
\end{example}

Generalized inequalities:

\begin{align*}
\min_x\quad & F_0(x) \\
s.t.\quad & F_i(x) \leq_{k_i} 0 \quad i = 1,...,m\\
& h_i(x) = 0\quad i = 1,...,m 
\end{align*}
where $F_0(x): \Re^n\rightarrow \Re$ convex, $F_i: \Re^n\rightarrow \Re^l$ "$K_i$-convex", $k_i$ are cones.

\begin{equation*}
	F_i(\lambda x + (1-\lambda)y)\leq_{k_i} \lambda F_i(x) + (1-\lambda)F_i(y)
\end{equation*}\\

Semi-definite program:
\begin{align*}
\min_x\quad & c^Tx \\
s.t.\quad & A_0+A_1x_1+,,,+A_nx_n\leq 0\quad \text{"LMI": linear matrix inequlity}\\
& Fx = g
\end{align*}
where $-(A_0+A_1x_1+,,,+A_nx_n)\in S^n$, $A_i\in S^n$, $\leq$ means they're PSD matrices.
%Above are notes for Nov 13


%Below are notes for Nov 18

\subsection{Second-Order Cone Program (SOCPs)}

\begin{align*}
\min \quad&F^Tx\\
s.t. \quad& \Vert A_ix+b_i\Vert_2 \leq c_i^Tx + d_i\quad i = 1,...,m\\
&Fx \leq g
\end{align*}
$A_i\in \Re^{n_i\times n}$, $x\in \Re^n$, $b_i\in \Re^{n_i}$, $F\in \Re^{p\times n}$, $g\in \Re^p$

Norm cone:
\begin{equation*}
\mathcal{C} = \{(x,t)\in \Re^{n+ 1} \vert \Vert x\Vert \leq t \} \subseteq \Re^{n+1}
\end{equation*}


\begin{marginfigure}
\centering
\includegraphics[width=1.8in,height=1.8in]{figures/ch09/figure1118_1.png}
%\caption{This is an inserted JPG graphic} 
%\label{fig:graph} 
\end{marginfigure}

\begin{itemize}
	\item First fix $t = t_0$, $\mathcal{C}_{t_0} = \{(x,t_0)\vert \Vert x\Vert \leq t_0 \}$, "fill-in" slice
	
	\item Next fix $x = x_0$, $\mathcal{C}_{x_0} = \{(x_0,t)\vert \Vert x_0\Vert \leq t \}$
	
	$\rightarrow$  Fix point in $x\in \Re^n$ to $x = x_0$ and "Fill up".
\end{itemize}


\begin{proof}
	1) Cone; 2) Convex cone
	
	\begin{enumerate}
		\item Pick any $(x_0, t_0)\in \mathcal{C}$, show that $(\theta x_0, \theta t_0)\in \mathcal{C} \quad\forall \theta \in \Re_+$:
		
		\begin{equation*}
		\Vert \theta x_0\Vert = \vert \theta \vert \Vert x_0\Vert = \theta \Vert x_0\Vert \leq \theta t_0
		\end{equation*}
		
		$\mathcal{C}$ is a cone
		
		\item Pick $(x_0, t_0)\in \mathcal{C}$ and $(y_0, s_0)\in \mathcal{C}$, show that:
		
		\begin{equation*}
		(\theta x_0 + (1-\theta)y_0, \theta t_0 + (1-\theta)s_0)\in \mathcal{C}, \forall 0\leq \theta \leq 1
		\end{equation*}
		
		\begin{align*}
		\Vert \theta x_0 + (1-\theta)y_0\Vert &\leq \Vert \theta x_0\Vert + \Vert (1-\theta)y_0\Vert\\
		&= \vert \theta \vert \Vert x_0\Vert + \vert (1-\theta) \vert \Vert y_0\Vert\\
		&= \theta \Vert x_0\Vert +  (1-\theta)\Vert y_0\Vert\\
		&\leq \theta t_0 + (1-\theta)s_0
		\end{align*}
		
		In summary, $\mathcal{C} = \{(x,t)\vert \Vert x\Vert_2 \leq t \}$ is a convex cone.
		
		Consider following affine map:
		\begin{equation*}
		F_i(x) = \begin{bmatrix}
		A_ix+b_i\\
		c_i^Tx+d_i
		\end{bmatrix}\in \Re^{n+ 1}
		\end{equation*}
		For ith constraint want $\Vert A_ix+b_i\Vert \leq c^T_ix+d_i$.
		
		\begin{equation*}
		\{x\vert F_i(x)\in \mathcal{C}_{n+ 1} = F_i^{-1}(\mathcal{C}_{n+ 1}) \}
		\end{equation*}
	\end{enumerate}
\end{proof}

Remarks:

\begin{enumerate}
	\item If all $A_i = 0$, then get an LP;
	
	\item If $c_i = 0$, then get a QP;
	
	\item Second-order because use $l_2$ norm for cone.
\end{enumerate}

\subsection{Robust Linear Programs}

\begin{align*}
\min \quad&c^Tx\\
s.t. \quad&a_i^Tx\leq b_i\quad i = 1,...,m
\end{align*}
$\Rightarrow$ Robust to uncertainty in $a_i$, two versions: (1) worst-case; (2)statistical.\\

(1) Worst Case:

Know $a_i\in \xi_i = \{a\vert a = \bar{a_i} + P_iu, \Vert u \Vert\leq 1 \} $. 

Then we can change:
\begin{align*}
\min\quad &c^Tx\\
s.t. \quad&a_i^Tx\leq b_i\quad a_i \in \xi_i \quad i = 1,...,m\\
&a_i\in\xi_i
\end{align*}

to:

\begin{align*}
\min \quad&c^Tx\\
s.t. \quad&(\bar{a_i} + P_iu)^Tx\leq b_i\quad i = 1,...,m\\
&\Vert u_i\Vert \leq 1
\end{align*}

\begin{align*}
\min\quad &c^Tx\\
s.t \quad&sup_{\Vert u_i\Vert \leq 1}(\bar{a_i} + P_iu)^Tx\leq b_i\quad i = 1,...,m
\end{align*}

\begin{align*}
(\bar{a_i} + P_iu)^Tx &= \bar{a_i}^Tx + u^TP_i^Tx \\
&\leq \bar{a_i}^Tx + (\frac{P_i^Tx}{\Vert P_i^Tx\Vert})^TP_i^Tx\\
&= \bar{a_i}^Tx + \frac{x^TP_iP_i^Tx}{\Vert P_i^Tx\Vert_2}\\
&= \bar{a_i}^Tx + \Vert P_i^Tx\Vert_2
\end{align*}\\

(2) Statistical approach

Model $a_i$ as random vectors $a_i \sim N(\bar{a_i}^T, \Sigma_i)$
\begin{align*}
E[a_i^Tx - b_i] &= \bar{a_i}^Tx - b_i = \mu_i\\
E[((a_i^Tx - b_i) - (a_i^Tx - b_i))^2] &= E[((a_i - \bar{a_i})^Tx)^2]\\
&= E[x^T(a_i - \bar{a_i})(a_i - \bar{a_i})^Tx] \\
&= x^TE[(a_i - \bar{a_i})(a_i - \bar{a_i})^T]x\\
&= x^T\Sigma_i x \\
&= \sigma^2\\
&= x^T \Sigma_i^{\frac{1}{2}}\Sigma_i^{\frac{1}{2}} x\\
&= \Vert \Sigma_i^{\frac{1}{2}} x\Vert^2_2
\end{align*}

\begin{align*}
Pr[a_i^T \leq b_i] &= Pr[a^T_i x\leq b_i]\\
&= \Phi(\frac{b_i - \bar{a_i}^Tx}{\sigma_i})\\
&= \Phi(\frac{b_i - \bar{a_i}^Tx}{\Vert \Sigma_i^{\frac{1}{2}} x\Vert_2})
\end{align*}

\begin{marginfigure}
\centering
\includegraphics[width=1.8in,height=1.8in]{figures/ch09/figure1118_2.png}
%\caption{This is an inserted JPG graphic} 
%\label{fig:graph} 
\end{marginfigure}

Say we want:

\begin{align*}
Pr[a_i^Tx\leq b_i]&\geq n\\
\Phi(\frac{b_i - \bar{a_i}^Tx}{\sigma_i})&\geq n\\
\frac{b_i - \bar{a_i}^Tx}{\sigma_i}&\geq \Phi^{-1}(m)
\end{align*}

Then it becomes:

\begin{align*}
\min \quad& c^Tx \\
s.t. \quad& b_i - \bar{a_i}^Tx\geq \Phi^{-1}(m)\Vert\Sigma_i^{\frac{1}{2}}x\Vert_2 \quad i = 1,...,m
\end{align*}

\subsection{Geometric Program(GP)}

\begin{itemize}
	\item "Monominials": $h(x) = cx_1^{\alpha_1}x_2^{\alpha_2}...x_n^{\alpha_n}$, $c>0, \alpha_i\in \Re$, $domh = (x\vert x_i>0 \quad \forall i\in {1...n})$
	
	\item "Posynomials": $F(x) = \sum^k_{k=1}c_kx_1^{\alpha_{1k}}x_2^{\alpha_{2k}}...x_n^{\alpha_{nk}}$(sum of monomials)
	
	$\rightarrow$ note closed under addition, multipilication, non-neg scalings.
\end{itemize}
GP: 

\begin{align*}
\min\quad &F_0(x)\\
s.t. \quad&F_i(x)\leq 1\quad i = 1,...,m\\
&h_i(x)= 1\quad i = 1,...,p
\end{align*}
where $F_0, F_1,...,F_m$ are posynomials, $h_0, h_1,...,h_m$ are monomials.

To get a GP into convex form, set $y_i = log x_i$, so $x_i = e^{y_i}$(recall $x_i > 0$)

\begin{align*}
h(x_1,x_2,...,x_m) &= cx_1^{\alpha_1}x_2^{\alpha_2}...x_n^{\alpha_n}\\
log h(x_1,x_2,...,x_m) &= logc + \alpha log x_1 +\alpha log x_2 + ... + \alpha_n log x_n\\
log h(e^{y_1},e^{y_2},...,e^{y_m}) &= logc + \alpha_1 log x_1 +\alpha_2 log x_2 + ... + \alpha_n log x_n\\
\end{align*}

Posynomials:

\begin{align*}
F(x_1,...,x_n) &= \sum^k_{i=1}c_kx_1^{\alpha_{1k}}x_2^{\alpha_{2k}}...x_n^{\alpha_{nk}}\\
log F(e^{y_1},e^{y_2},...,e^{y_n}) &= log(\sum^k_{k=1}e^{logc_k}e^{\alpha_{1k}y_1}\cdots e^{\alpha_{nk}y_n})\\
&=log(\sum^k_{k=1}e^{\alpha_{1k}y_1+\cdots+\alpha_{nk}y_n+logc_k})
\end{align*}

$\Rightarrow$ Show a convex function of $y$

GP in convex form:

\begin{align*}
\min\quad & logF_0(e^{y_1},e^{y_2},...,e^{y_n}) \\
s.t. \quad& logF_1(e^{y_1},e^{y_2},...,e^{y_n}) \leq log(1) = 0\\
&log h_i(e^{y_1},e^{y_2},...,e^{y_n}) = 0
\end{align*}
%Above are notes for Nov 18



%Below are notes for Nov 20

%Above are notes for Nov 18
