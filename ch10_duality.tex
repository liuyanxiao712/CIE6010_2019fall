%% Placeholder for discussion of duality

%Below are notes for Nov 20
In optimization theory, duality or the duality principle is the principle that optimization problems may be viewed from either of two perspectives, that is, the primal problem or the dual problem.

Let's consider the primal problem formulated as follows,
\begin{align*}
\min \quad&F_0(x) \\
s.t. \quad&F_i(x)\leq 0, i = 1,...,m\\
&h_i(x)= 0, i = 1,...,p
\end{align*}
Note that we do not have any assumptions of convexity here.

So the feasible set for this problem is
$$D = (\cap^m_{i=1}\text{dom}\ F_i)\cap(\cap^p_{i=1}\text{dom}\ h_i)$$ 
and the optimal value is $p^*$, optimal variable is $x^*$.

\begin{definition}[The Lagrangian Function]
	We define the Lagrangian function as follows,
	$$L(x,\lambda,\nu) := F_0(x) + \sum^m_{i=1}\lambda_i F_i(x) + \sum^p_{i=1}\nu h_i(x)$$
	where
	$$\lambda =
	\begin{bmatrix}
		\lambda_1\\
		\lambda_2\\
		\vdots\\
		\lambda_m
	\end{bmatrix},\
	\nu = 
	\begin{bmatrix}
		\nu_1\\
		\nu_2\\
		\vdots\\
		\nu_m
	\end{bmatrix}$$

The pairs $(\lambda, \nu)$ are called the "Lagrange multipliers" or "dual variables", and the domain for the Lagrangian function is given by 
$$\text{dom}\ L = D\times \Re^m \times \Re^p$$
\end{definition}

\begin{definition}[The "dual" function]
	The dual function $g(\cdot, \cdot)$ is defined as 
	\begin{equation*}
	g(\lambda, \nu) = \min_{x\in D}\quad L(x,\lambda,\nu)
	\end{equation*}
	
	Note: removes dependence on $x$.
\end{definition}


\begin{definition}{The dual optimization problem}
	\begin{align*}
	\max_{\lambda, \nu} \quad&g(\lambda, \nu) \\
	s.t. \quad&\lambda \geq 0
	\end{align*}

Note: $\nu_i$ are unconstrained, and we denote the optimal value as $d^*$, optimal dual variables as $\lambda^*$ and $\nu^*$.
\end{definition}



Duality Theory: For most convex optimization problem, $d^* = p^*$.\\


\begin{align*}
\min \quad&F_0(x) \\
s.t. \quad&F_i(x)\leq 0, \quad i = 1,...,m \\
&h_i(x)= 0, \quad i = 1,...,p
\end{align*}

\begin{equation*}
L(x,\lambda,\nu) = F_0(x) + \sum^m_{i=1}\lambda_i F_i(x) + \sum^p_{i=1}\nu_i h_i(x)
\end{equation*}

Dual Function:

\begin{equation*}
g(\lambda, \nu) = \min_{x\in D}\quad L(x,\lambda,\nu) 
\end{equation*}


\begin{align*}
\max \quad&g(\lambda, \nu) \\
s.t. \quad&\lambda \geq 0
\end{align*}

(1) $g(\lambda, \nu)$ is concave in $(\lambda, \nu)$ for all $F_0,...,F_m$, $h_0,...,h_p$.

\begin{equation*}
g(\lambda, \nu) =\min_{x\in D}[F_0(x) + \sum^m_{i=1}\lambda_iF_i(x) + \sum^p_{i=1}\nu_ih_i(x) ]
\end{equation*}


(2) For any:

\begin{enumerate}
	\item Primal feasible $x$ (ie. $F_i(x)\leq 0, \quad i = 1,...,m$, $h_i(x)= 0, \quad i = 1,...,p$
	
	\item Dual feasible $(\lambda, \nu)$ (ie. $\lambda \geq 0$)
	
	\begin{equation*}
	g(\lambda, \nu)\leq F_0(x) \qquad (x, \lambda, \nu) \in \mathcal{C}\times \Re^m_+\times \Re^p
	\end{equation*}
\end{enumerate}

\begin{proof}
	\begin{align*}
	F_0(x) &\geq F_0(x) + \sum^m_{i=1}\lambda_iF_i(x) + \sum^p_{i=1}\nu_ih_i(x)\\
	&\geq \min_{x\in D} [F_0(x) + \sum^m_{i=1}\lambda_iF_i(x) + \sum^p_{i=1}\nu_ih_i(x) ] = g(\lambda, \nu)
	\end{align*}
\end{proof}

The point of greatest interest is $x^*$ where $p^* = F_0(x^*)$.

Plug in: 
\begin{equation*}
p^* = F_0(x^*) \geq g(\lambda, \nu), \quad \forall \text{dual-feasible} (\lambda, \nu)\rightarrow \lambda \geq 0
\end{equation*}

Optimize over $(\lambda, \nu)$ where $(\lambda \geq 0)$ to maintain dual feasibility to get greatest lower bound. 

\begin{equation*}
p^* = F_0(x^*) \geq g(\lambda^*, \nu^*) = d^*
\end{equation*}

$p^* - d^*(\text{optimal duality gap})\geq 0$ $\rightarrow$ "weak-duality".

(4) For convex primal optimization problems, (i.e. $F_i(x)$ convex and $h_i(x)$ affine) and under certain conditions called "constraint qualification"(i.e. not all constraint sets allowed) then $p^* = d^*$ $\rightarrow$ "strong duality"

Slater's conditions: A set of constrains $F_i(x) \leq 0$, $Ax = b$ satisfies Slater's if $\exists x\in D$ s.t. $F_i(x)<0,(\text{feasible set has an interior}) \forall i = 1,...,m$ and $Ax = b$


%Above are notes for Nov 20

